{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/Geospatial_Workshops/blob/main/notebooks/detecto_geospatial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with Detecto\n",
        "\n",
        "This notebook uses [detecto](https://detecto.readthedocs.io/en/latest/), a lightweight Python library that will allow us to create a convolutional neural network object detection model in very few lines of code. It downloads a pre-trained Faster R-CNN ResNet50 FPN from Pytorch as the starting point. However, the pre-trained model is not specifically trained to identify objects from aerial imagery. We have to train it to do so. In this notebook, we will use an existing labeled training dataset to fine-tune the model to identify 10 different objects from aerial imagery."
      ],
      "metadata": {
        "id": "Q5TyMhfTgUJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the required libraries to the virtual machine (! indicates install to underlying computer)\n",
        "!pip install detecto --quiet\n",
        "!pip install pascal-voc-writer --quiet"
      ],
      "metadata": {
        "id": "bXsjqKfzggdM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the pre-trained model on some test imagery\n",
        "\n",
        "In general, the pre-trained model should NOT be able to identify objects very well from an aerial view. It has not yet been trained to do so."
      ],
      "metadata": {
        "id": "nxHDbIkksNyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Get a test image and run the pre-trained model on it. It should NOT produce good results.\n",
        "\n",
        "#Import necessary modules from Detecto python library\n",
        "from detecto.core import Model #bring in the Faster R-CNN ResNet50 FPM model\n",
        "from detecto.utils import read_image\n",
        "from detecto.visualize import show_labeled_image\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ua-datalab/Geospatial_Workshops/main/data/vehicle_test2.png\n",
        "image = read_image('vehicle_test2.png')\n",
        "\n",
        "model = Model() #assign the pre-trained model to a variable\n",
        "labels, boxes, scores = model.predict(image)\n",
        "show_labeled_image(image, boxes, labels)"
      ],
      "metadata": {
        "id": "v_K_XjbVspyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a test image and run the pre-trained model on it. It should NOT produce good results.\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ua-datalab/Geospatial_Workshops/main/data/baseball_field_test.png\n",
        "\n",
        "image = read_image('baseball_field_test.png')\n",
        "labels, boxes, scores = model.predict(image)\n",
        "show_labeled_image(image, boxes, labels)"
      ],
      "metadata": {
        "id": "c4KsjNECs6at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Training Dataset\n",
        "The [NWPU VHR-10 dataset](https://github.com/chaozhong2010/VHR-10_dataset_coco) is a very high resolution (VHR) aerial imagery dataset that consists of 800 total images. The dataset has ten classes of labeled objects: 1. airplane(757), 2. ship(302), 3. storage tank(655), 4. baseball diamond(390), 5. tennis court(524), 6. basketball court(159), 7. ground track field(163), 8. harbor(224), 9. bridges(124), and 10. vehicle(477)."
      ],
      "metadata": {
        "id": "OkNk3ZofN2MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the labeled training data set (NWPU_VHR-10.tar) from github. It will be located in the `/content` directory in Colab\n",
        "!wget https://github.com/ua-datalab/Geospatial_Workshops/raw/main/data/NWPU_VHR-10.tar"
      ],
      "metadata": {
        "id": "J9eKYFr0j-QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The next line of code will decompress the tar file and give you access to the imagery and labels.\n",
        "\n",
        "Within the root directory (NWPU_VHR-10_dataset), the subdirectory *positive_image_set* has 650 jpg images with each image containing at least one target object. The subdirectory *ground_truth* contains a a text file corresponding to each image. Each line of those text files defines a ground truth bounding box in the following format:\n",
        "(x1,y1),(x2,y2),a\n",
        "\n",
        "where (x1,y1) denotes the top-left coordinate of the bounding box, (x2,y2) denotes the right-bottom coordinate of the bounding box, and a is the object class (1-airplane, 2-ship, 3-storage tank, 4-baseball diamond, 5-tennis court, 6-basketball court, 7-ground track field, 8-harbor, 9-bridge, 10-vehicle)."
      ],
      "metadata": {
        "id": "MUe8IPsdM9OX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPqS8yLSatt4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Decompress the labeled training data tar file\n",
        "!tar -xvf NWPU_VHR-10.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert labeled dataset to Pascal VOC Format\n",
        "\n",
        "**detecto** requires that the dataset is labeled in the [Pascal VOC format](http://host.robots.ox.ac.uk/pascal/VOC/). In this dataset, the labels for the images are stored in plain text files, so we will first convert the data to the required format.\n"
      ],
      "metadata": {
        "id": "aD8-jqa9iMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the required tools\n",
        "\n",
        "import os # used for looking through directories\n",
        "import re # search in strings\n",
        "from pascal_voc_writer import Writer # create the .xml files with the VOC format\n",
        "from PIL import Image # used to get height, width of images"
      ],
      "metadata": {
        "id": "hDv68QLMkQrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a list of the text file names\n",
        "base = './NWPU_VHR-10_dataset'\n",
        "txt_files = os.listdir(os.path.join(base, 'ground_truth'))"
      ],
      "metadata": {
        "id": "YkECFa6qjuhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary that maps the class number to the class name.\n",
        "\n",
        "classes = {\n",
        "    1: 'airplane',\n",
        "    2: 'ship',\n",
        "    3: 'storage tank',\n",
        "    4: 'baseball diamond',\n",
        "    5: 'tennis court',\n",
        "    6: 'basketball court',\n",
        "    7: 'ground track field',\n",
        "    8: 'harbor',\n",
        "    9: 'bridge',\n",
        "    10: 'vehicle'\n",
        "}\n",
        "\n",
        "print(type(classes))"
      ],
      "metadata": {
        "id": "7KqwY2MNk45X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through each ground truth text file and convert to Pascal VOC format.\n",
        "# This will create a new xml file for each image and will save it in the directory 'positive_image_set'\n",
        "for txt in txt_files:\n",
        "    # open the text file for reading\n",
        "    with open(os.path.join(base, 'ground_truth', txt), 'r') as reader:\n",
        "\n",
        "        # locate and open corresponding image\n",
        "        img_id, _ = txt.split('.')\n",
        "        img_path = os.path.join(base, 'positive_image_set', f'{img_id}.jpg')\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        # create a writer for the xml file\n",
        "        xml_path = os.path.join(base, 'positive_image_set', f'{img_id}.xml')\n",
        "        writer = Writer(img_path, img.width, img.height)\n",
        "\n",
        "        # convert the text file to an xml file and save\n",
        "        lines = reader.readlines()\n",
        "        for line in lines:\n",
        "            numbers = re.findall(r'\\d+', line)\n",
        "            if not numbers: # skip empty lines\n",
        "                continue\n",
        "            xmin, ymin, xmax, ymax, class_id = list(map(int, numbers))\n",
        "            writer.addObject(classes[class_id], xmin, ymin, xmax, ymax)\n",
        "        writer.save(xml_path)"
      ],
      "metadata": {
        "id": "xGKgXw6cpfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load training images into Datasets\n",
        "Next, we load the 650 images into 'datasets'\n"
      ],
      "metadata": {
        "id": "1bz5xSY_hv6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module\n",
        "from detecto.core import Dataset\n",
        "\n",
        "# create the datasets\n",
        "dataset = Dataset(os.path.join(base, 'positive_image_set/'))"
      ],
      "metadata": {
        "id": "Hc22o1I73kOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a random split of training and validation data from the dataset"
      ],
      "metadata": {
        "id": "iZUUn8PVlCyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary modules from pytorch\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# this sets the seed of the random generator to ensure reproducibility\n",
        "random_generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "# Split the dataset into 80% training and 20% validation\n",
        "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "aLuANhuJkN3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the training images and labels\n",
        "**detecto** provides a visualization function that will display the image along with bounding boxes and labels for the objects it contains."
      ],
      "metadata": {
        "id": "bhbPV0vUcshL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detecto.visualize import show_labeled_image\n",
        "\n",
        "# show some of the training data. Change the [number] to other numbers to see other images.\n",
        "image, targets = train_dataset[509]\n",
        "show_labeled_image(image, targets['boxes'], targets['labels'])"
      ],
      "metadata": {
        "id": "U8yqgC1Y6OJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also visualize the validation dataset\n",
        "image, targets = val_dataset[1]\n",
        "show_labeled_image(image, targets['boxes'], targets['labels'])"
      ],
      "metadata": {
        "id": "xaEzYYfhWAZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "With the data loaded, we can now train the model. Here, we use a DataLoader to wrap the Dataset object. This allows us to specify the batch size used during each training step, one of a few hyperparameters we can play with during fine tuning.\n",
        "\n",
        "Note that we are required to tell **detecto** which labels we are interested in learning to identify."
      ],
      "metadata": {
        "id": "nsnKvKLsnCOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary modules\n",
        "from detecto.core import DataLoader, Model\n",
        "\n",
        "# wrap the training set in a DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# specify all unique labels you're trying to predict\n",
        "labels = [\n",
        "    'airplane',\n",
        "    'ship',\n",
        "    'storage tank',\n",
        "    'baseball diamond',\n",
        "    'tennis court',\n",
        "    'basketball court',\n",
        "    'ground track field',\n",
        "    'harbor',\n",
        "    'bridge',\n",
        "    'vehicle'\n",
        "]"
      ],
      "metadata": {
        "id": "6aLvT89TVzlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##TRAINING RUN!!\n",
        "# initialize a model with the target labels and fit the model\n",
        "# This step will take some time and is accelerated with GPU\n",
        "model = Model(labels)\n",
        "losses = model.fit(train_loader,\n",
        "                   val_dataset,\n",
        "                   epochs=8,\n",
        "                   lr_step_size=5,\n",
        "                   learning_rate=0.001,\n",
        "                   verbose=True)"
      ],
      "metadata": {
        "id": "DouEUfNwW8Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we perform a sanity check to ensure that the training process is working correctly by checking that the loss is decreasing."
      ],
      "metadata": {
        "id": "zRsJ8YkenrlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JFRaVSlnmB-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Intersection over Union (IOU) to assess 'accuracy' of object detection.\n"
      ],
      "metadata": {
        "id": "0QMkR_Cco7hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code block will calculate IOU for each validation/prediction pair.\n",
        "#A thresold of 0.5 has been set where an IOU value above 0.5 is considered a good prediction\n",
        "!pip install torchvision\n",
        "from torchvision.ops import box_iou\n",
        "from detecto.core import Dataset\n",
        "from detecto.utils import normalize_transform\n",
        "from detecto.visualize import show_labeled_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_accuracy(model, val_dataset, iou_threshold=0.5):\n",
        "  \"\"\"\n",
        "  Calculates the accuracy of the model on the validation dataset using torchvision.ops.box_iou.\n",
        "\n",
        "  Args:\n",
        "      model: The trained detecto model.\n",
        "      val_dataset: The validation dataset.\n",
        "      iou_threshold: The IoU threshold for considering a prediction correct.\n",
        "\n",
        "  Returns:\n",
        "      The accuracy of the model.\n",
        "  \"\"\"\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "\n",
        "  for image, targets in tqdm(val_dataset):\n",
        "      # Convert target boxes to a PyTorch tensor\n",
        "      target_boxes = targets['boxes']\n",
        "\n",
        "      labels, boxes, scores = model.predict(image)\n",
        "\n",
        "      # Calculate IoU for all pairs of boxes\n",
        "      iou_matrix = box_iou(target_boxes, boxes)\n",
        "\n",
        "      for i in range(len(target_boxes)):\n",
        "          gt_label = targets['labels'][i]\n",
        "\n",
        "          # Get the IoU for the current ground truth box with all predicted boxes\n",
        "          iou_row = iou_matrix[i]\n",
        "\n",
        "          # Find the best matching predicted box\n",
        "          best_iou, best_idx = torch.max(iou_row, dim=0)\n",
        "\n",
        "          if best_iou >= iou_threshold:\n",
        "              pred_label = labels[best_idx]\n",
        "              if pred_label == gt_label:\n",
        "                  correct_predictions += 1\n",
        "\n",
        "          total_predictions += 1\n",
        "\n",
        "  return correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "# Example usage (assuming you have model and val_dataset defined)\n",
        "accuracy = calculate_accuracy(model, val_dataset)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Scts72Z-9Nys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model to Colab disk"
      ],
      "metadata": {
        "id": "DjWMFtlP-2Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save model Colab disk\n",
        "model.save('detecto_vhr_finetune.pth')"
      ],
      "metadata": {
        "id": "nIOp5LdKCr1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip the Training, and use an existing fine-tuned model\n",
        "\n",
        "If you have computation limitation and want to skip the model training, we can use a pre-trained model that has already been trained on the NWPU-VHR dataset. Download the model from huggingface."
      ],
      "metadata": {
        "id": "FV-i_U7tfFlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the fine-tuned model from Hugging Face\n",
        "!wget https://huggingface.co/jgillan/detecto_vhr_finetune/resolve/main/detecto_vhr_finetune.pth\n",
        "\n",
        "\n",
        "from detecto.core import Model\n",
        "\n",
        "#Load fine-tuned model into our session\n",
        "model = Model.load('detecto_vhr_finetune.pth', labels)"
      ],
      "metadata": {
        "id": "36kD5EdMTrFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visually Look at Validation Results"
      ],
      "metadata": {
        "id": "gHyK6OJJn1eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [] #create an empty list\n",
        "for i in range(8,12): #  create a for loop that will grab 4 images from the validation dataset\n",
        "    image, _ = val_dataset[i]\n",
        "    images.append(image)\n",
        "\n",
        "top_predictions= model.predict_top(images) # Predict objects on the 4 validation images"
      ],
      "metadata": {
        "id": "e8pD5LsH8c5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display 4 validation images and show the predicted bounding box and label.\n",
        "\n",
        "from detecto.visualize import plot_prediction_grid\n",
        "\n",
        "plot_prediction_grid(model, images, dim=(2, 2), figsize=(8, 8))"
      ],
      "metadata": {
        "id": "Z87jqBXrmWiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the fine-tuned model on independent images"
      ],
      "metadata": {
        "id": "-HazvMwV0-vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary modules\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import IPython.display as display\n",
        "\n",
        "# Replace the URL with the actual link to your .png file\n",
        "url = 'https://raw.githubusercontent.com/ua-datalab/Geospatial_Workshops/main/data/airplane_test.png'\n",
        "urllib.request.urlretrieve(url, 'airplane_test.png')\n",
        "\n",
        "# Load and display the image\n",
        "image = Image.open('airplane_test.png')\n",
        "display.display(image)\n"
      ],
      "metadata": {
        "id": "KCT6MZ_Kt15w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use the fine-tuned model on the independent image\n",
        "image = read_image('airplane_test.png')\n",
        "labels, boxes, scores = model.predict(image)\n",
        "show_labeled_image(image, boxes, labels)"
      ],
      "metadata": {
        "id": "aLanWQiAyZEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use the fine-tuned model on the independent image\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ua-datalab/Geospatial_Workshops/main/data/vehicle_test2.png\n",
        "\n",
        "image = read_image('vehicle_test2.png')\n",
        "labels, boxes, scores = model.predict(image)\n",
        "show_labeled_image(image, boxes, labels)"
      ],
      "metadata": {
        "id": "47qzm0Tj4S3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use the fine-tuned model on the independent image\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ua-datalab/Geospatial_Workshops/main/data/baseball_field_test.png\n",
        "\n",
        "image = read_image('baseball_field_test.png')\n",
        "labels, boxes, scores = model.predict(image)\n",
        "show_labeled_image(image, boxes, labels)"
      ],
      "metadata": {
        "id": "697vnL9vGf0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}