{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/Geospatial_Workshops/blob/main/notebooks/deepforest_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d05c59d-33da-44e3-a211-8efef2062846",
      "metadata": {
        "id": "3d05c59d-33da-44e3-a211-8efef2062846"
      },
      "source": [
        "## Tree Detection with DeepForest\n",
        "\n",
        "This jupyter notebook uses the python library DeepForest to identify and put bounding boxes around trees.\n",
        "\n",
        "If using the software, please cite as:\n",
        "Geographic Generalization in Airborne RGB Deep Learning Tree Detection Ben. G. Weinstein, Sergio Marconi, Stephanie A. Bohlman, Alina Zare, Ethan P. White bioRxiv 790071; doi: https://doi.org/10.1101/790071\n",
        "\n",
        "Documentation for DeepForest can be found at https://deepforest.readthedocs.io/en/latest/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69370d7e-bcea-499b-b3fd-605389f1d5a1",
      "metadata": {
        "id": "69370d7e-bcea-499b-b3fd-605389f1d5a1"
      },
      "outputs": [],
      "source": [
        "#Install the deepforest python library. After installing, you may need to restart the kernel before moving to the next code snippet\n",
        "!pip install DeepForest --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a0e049-5661-4b76-a0c6-21377d9110be",
      "metadata": {
        "id": "c2a0e049-5661-4b76-a0c6-21377d9110be"
      },
      "outputs": [],
      "source": [
        "##After restarting the kernel, import libraries into environment...\n",
        "from deepforest import main\n",
        "from deepforest import get_data\n",
        "from deepforest.utilities import boxes_to_shapefile\n",
        "from deepforest.utilities import shapefile_to_annotations\n",
        "from deepforest.preprocess import split_raster\n",
        "from deepforest.visualize import plot_predictions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import numpy\n",
        "import rasterio\n",
        "import geopandas as gpd\n",
        "from rasterio.plot import show\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6742c0c-5c29-46eb-bd60-534ddecb307a",
      "metadata": {
        "id": "b6742c0c-5c29-46eb-bd60-534ddecb307a"
      },
      "outputs": [],
      "source": [
        "#Bring a DeepForest pretrained model into environment. It is trained to identify trees from aerial imagery\n",
        "#It is located at https://github.com/weecology/DeepForest/releases/tag/1.0.0\n",
        "model = main.deepforest()\n",
        "model.use_release()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27819458-7055-4330-ab31-a8e6be7e25d6",
      "metadata": {
        "id": "27819458-7055-4330-ab31-a8e6be7e25d6"
      },
      "source": [
        "## Predict Tree Crowns on Raw (non-georeferenced images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Path for the image you want to ID trees.\n",
        "#These are non-georeferenced single jpeg drone image located in Cyverse datastore\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/commons/cyverse_curated/Gillan_Ecosphere_2021/raw_images/May_2019/15-g2/100_0123/100_0123_0086.JPG\n",
        "image_path = get_data(\"/content/100_0123_0086.JPG\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/DJI_0184.jpeg\n",
        "image_path2 = get_data(\"/content/DJI_0184.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/100_0407_0064.jpeg\n",
        "image_path3 = get_data(\"/content/100_0407_0064.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/DJI_0468.jpeg\n",
        "image_path4 = get_data(\"/content/DJI_0468.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/101_0472_0074.jpeg\n",
        "image_path5 = get_data(\"/content/101_0472_0074.jpeg\")\n"
      ],
      "metadata": {
        "id": "YK8NqKlO_qm_"
      },
      "id": "YK8NqKlO_qm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5e2b9f-0501-4bff-bd9e-64eeb0ef7c3e",
      "metadata": {
        "id": "cd5e2b9f-0501-4bff-bd9e-64eeb0ef7c3e"
      },
      "outputs": [],
      "source": [
        "#Identify and put bounding boxes around all trees in the image\n",
        "#This will create a table showing image coordinates of every predicted tree\n",
        "#The 'score' is the confidence that the prediction is correct. Values closer to 1 are better.\n",
        "trees = model.predict_image(path=image_path3, return_plot = False)\n",
        "trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99105ef-36ed-420e-aed4-0b83eb7608c9",
      "metadata": {
        "id": "e99105ef-36ed-420e-aed4-0b83eb7608c9"
      },
      "outputs": [],
      "source": [
        "#Show the image with the bounding boxes\n",
        "plot = model.predict_image(path=image_path3, return_plot = True, color=(0, 255, 255), thickness=6)\n",
        "plt.imshow(plot[:,:,::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c01de3a-c409-48db-aa3e-9d42239e327f",
      "metadata": {
        "id": "5c01de3a-c409-48db-aa3e-9d42239e327f"
      },
      "source": [
        "## Predict Tree Crowns on Georeferenced Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff50dfc-dd00-43ef-8bc8-c5834427e5fc",
      "metadata": {
        "id": "4ff50dfc-dd00-43ef-8bc8-c5834427e5fc"
      },
      "outputs": [],
      "source": [
        "#Set the path for a georeferenced image you want to predict tree crowns\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/hole_17_ortho_utm.tif\n",
        "raster_path = get_data(\"/content/hole_17_ortho_utm.tif\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b6a04e-4ca0-4d51-9982-aa47f93e11d0",
      "metadata": {
        "id": "c5b6a04e-4ca0-4d51-9982-aa47f93e11d0"
      },
      "outputs": [],
      "source": [
        "##Predict tree crowns on a georeferenced image\n",
        "predicted_raster = model.predict_tile(raster_path, return_plot = True, patch_size=1000, patch_overlap=0.25, color=(255, 255, 0), thickness=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a53f0a-ca24-4e82-875f-1a69b7c869e7",
      "metadata": {
        "id": "d8a53f0a-ca24-4e82-875f-1a69b7c869e7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(predicted_raster)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce5727a-1bba-4f45-b982-2fbd82285ff8",
      "metadata": {
        "id": "7ce5727a-1bba-4f45-b982-2fbd82285ff8"
      },
      "source": [
        "## Improve Model with Training\n",
        "If the pre-trained model does not identify all trees correclty, then we want to improve the model by adding some training data and fine-tuning the model.\n",
        "Manual labeling of trees (bounding boxes) can be done in QGIS. The output should be a polygon shapefile (.shp). Instructions for using QGIS is [here](https://github.com/ua-datalab/Geospatial_Workshops/wiki/Drone-Image-Analysis-%E2%80%90-Deep-Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acd9b1b-e735-40ba-a8d1-4287dfdcd4f9",
      "metadata": {
        "id": "3acd9b1b-e735-40ba-a8d1-4287dfdcd4f9"
      },
      "outputs": [],
      "source": [
        "#Define data paths in preparation to convert .shp to annotation dataframe that can be used for training\n",
        "## I have found that it is important that the orthomosaic and shapefiles used should have map projections (e.g., UTM). Otherwise, there will be a shift problem in the `shapefile_to_annotations` step.\n",
        "\n",
        "# training data imagery path\n",
        "# We are using the golf course orthomosaic geotiff\n",
        "train_image_path = get_data(\"/content/hole_17_ortho_utm.tif\")\n",
        "\n",
        "# the directory that has the training data imagery\n",
        "train_image_dir = os.path.dirname(train_image_path)\n",
        "\n",
        "# the name of the training imagery\n",
        "image_name = os.path.basename(train_image_path)\n",
        "\n",
        "# shapefile path\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/golf_train_utm.shp\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/golf_train_utm.shx\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/golf_train_utm.dbf\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/golf_train_utm.prj\n",
        "\n",
        "shp_path = \"/content/golf_train_utm.shp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7471ca25-5d5b-472f-8d2d-cc65fdcc7076",
      "metadata": {
        "id": "7471ca25-5d5b-472f-8d2d-cc65fdcc7076"
      },
      "outputs": [],
      "source": [
        "## Show shapefile overlayed on orthomosaic\n",
        "\n",
        "# Open the GeoTIFF file\n",
        "with rasterio.open(train_image_path) as src:\n",
        "    fig, ax = plt.subplots(figsize=(20, 20))\n",
        "    show(src, ax=ax)\n",
        "\n",
        "    # Read the shapefile\n",
        "    shapefile = gpd.read_file(shp_path)\n",
        "\n",
        "    # Plot the shapefile on top of the GeoTIFF\n",
        "    shapefile.plot(ax=ax, facecolor='none', edgecolor='yellow')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4522e0-5baf-40e0-8ce7-78b7b0c390a5",
      "metadata": {
        "id": "0a4522e0-5baf-40e0-8ce7-78b7b0c390a5"
      },
      "outputs": [],
      "source": [
        "##Convert .shp (shapefile) to annoation that can be ingested by DeepForest\n",
        "savedir = \"/content\"\n",
        "df = shapefile_to_annotations(shapefile=shp_path, rgb=train_image_path, geometry_type='bbox', savedir=savedir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a3cd0f-f795-4f0a-87c6-dad756cfebb2",
      "metadata": {
        "id": "23a3cd0f-f795-4f0a-87c6-dad756cfebb2"
      },
      "outputs": [],
      "source": [
        "#Write training annotation dataframe to csv file\n",
        "df.to_csv(os.path.join(savedir, \"labels_pixel_coords.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b63fe9f-9b15-4ea8-8fb8-560840eec319",
      "metadata": {
        "id": "7b63fe9f-9b15-4ea8-8fb8-560840eec319"
      },
      "outputs": [],
      "source": [
        "#Show the annotation\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2f6aa8-c97a-4012-82ce-d7f30af8d3a4",
      "metadata": {
        "scrolled": true,
        "id": "4c2f6aa8-c97a-4012-82ce-d7f30af8d3a4"
      },
      "outputs": [],
      "source": [
        "## Display the annotation on the orthomosaic\n",
        "\n",
        "rasterio_src = rasterio.open(train_image_path)\n",
        "\n",
        "image = rasterio_src.read()\n",
        "image = numpy.rollaxis(image, 0, 3)\n",
        "\n",
        "fig = plot_predictions(image, df, color=(255, 255, 0), thickness=20)\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8500b69-8b50-4358-b683-8aa38728030d",
      "metadata": {
        "id": "e8500b69-8b50-4358-b683-8aa38728030d"
      },
      "outputs": [],
      "source": [
        "##This will split a large georeferenced image (and it's labels) into smaller pieces. This prevents running out of memory.\n",
        "annotation_path = os.path.join(savedir, \"labels_pixel_coords.csv\")\n",
        "\n",
        "#create a directory where the smaller images will be stored\n",
        "crop_dir = os.path.join(savedir, 'train_data')\n",
        "\n",
        "# Do the split and write out the cropped images as .png files.\n",
        "#Also write a new annotation table (csv) that lists all of the label coordinates and the cropped image they belong to.\n",
        "output_crops = split_raster(path_to_raster=train_image_path,\n",
        "                            annotations_file=annotation_path,\n",
        "                            base_dir=crop_dir,\n",
        "                            patch_size=1100,  #1100x1100 pixels\n",
        "                            patch_overlap=0.25, #cropped image overlap. This is useful because label boxes may be on the edge of cropped images.\n",
        "                            allow_empty=False)\n",
        "\n",
        "print(f\"Number of tree crown annotations: {len(output_crops)}\")\n",
        "output_crops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c55fd6-6309-4e27-b5b3-6c296a862990",
      "metadata": {
        "id": "38c55fd6-6309-4e27-b5b3-6c296a862990"
      },
      "source": [
        "### Split annotation data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e007f55f-c4a6-460b-ba99-0b15ec537e20",
      "metadata": {
        "id": "e007f55f-c4a6-460b-ba99-0b15ec537e20"
      },
      "outputs": [],
      "source": [
        "#identify all of the cropped images as an array\n",
        "image_paths = output_crops.image_path.unique()\n",
        "\n",
        "#Of the unique cropped image paths, randomly select 25% of them\n",
        "validation_paths = numpy.random.choice(image_paths, int(len(image_paths)*0.25))\n",
        "\n",
        "#Get the individual tree annotation from the 25% cropped images\n",
        "validation_annotations = output_crops.loc[output_crops.image_path.isin(validation_paths)]\n",
        "\n",
        "#Get the individual tree annotations from the remaining 75% cropped images\n",
        "train_annotations = output_crops.loc[~output_crops.image_path.isin(validation_paths)]\n",
        "\n",
        "#Print out the number of training and testing tree crown annotations\n",
        "train_annotations.head()\n",
        "print(\"There are {} training crown annotations\".format(train_annotations.shape[0]))\n",
        "print(\"There are {} test crown annotations\".format(validation_annotations.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d5d357-199d-4340-98bc-edcbd660369d",
      "metadata": {
        "id": "b5d5d357-199d-4340-98bc-edcbd660369d"
      },
      "outputs": [],
      "source": [
        "## Write training and validation annotations to separate csv files\n",
        "\n",
        "#save to file and create the file dir\n",
        "training_file= os.path.join(crop_dir,\"train.csv\")\n",
        "validation_file= os.path.join(crop_dir,\"valid.csv\")\n",
        "#Write window annotations file without a header row, same location as the \"base_dir\" above.\n",
        "train_annotations.to_csv(training_file,index=False)\n",
        "validation_annotations.to_csv(validation_file,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c8ac15-e8ed-4ffa-97c0-5ec1963cfae1",
      "metadata": {
        "id": "61c8ac15-e8ed-4ffa-97c0-5ec1963cfae1"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c93d670-efc9-4a47-bca8-7157425ba6b2",
      "metadata": {
        "id": "4c93d670-efc9-4a47-bca8-7157425ba6b2"
      },
      "outputs": [],
      "source": [
        "##Set parameters for the training run\n",
        "\n",
        "#Define the pre-trained model\n",
        "model = main.deepforest()\n",
        "\n",
        "model.config['gpus'] = '-1' #move to GPU and use all the GPU resources\n",
        "\n",
        "#model.config[\"save-snapshot\"] = False\n",
        "#model.config[\"train\"][\"fast_dev_run\"] = True\n",
        "\n",
        "#The annotation table\n",
        "model.config[\"train\"][\"csv_file\"] = training_file\n",
        "#The directory where the training imagery is located\n",
        "model.config[\"train\"][\"root_dir\"] = os.path.dirname(training_file)\n",
        "\n",
        "model.config[\"score_thresh\"] = 0.4\n",
        "model.config[\"train\"]['epochs'] = 4\n",
        "\n",
        "model.config[\"validation\"][\"csv_file\"] = validation_file\n",
        "model.config[\"validation\"][\"root_dir\"] = os.path.dirname(validation_file)\n",
        "\n",
        "\n",
        "model.create_trainer()\n",
        "\n",
        "model.use_release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5f8efb-a7e2-4c75-acc4-ea5ebcb0dc50",
      "metadata": {
        "id": "6d5f8efb-a7e2-4c75-acc4-ea5ebcb0dc50"
      },
      "outputs": [],
      "source": [
        "##TRAIN THE MODEL!\n",
        "#You can watch the GPU usage by using nvtop (sudo apt install nvtop)\n",
        "start_time = time.time()\n",
        "model.trainer.fit(model)\n",
        "print(f\"--- Training on GPU: {(time.time() - start_time):.2f} seconds ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7478be6e-2f32-4c1e-ab35-05a102e2299f",
      "metadata": {
        "id": "7478be6e-2f32-4c1e-ab35-05a102e2299f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d425f1a4-b4b7-4887-8c78-f97a0ff42794",
      "metadata": {
        "id": "d425f1a4-b4b7-4887-8c78-f97a0ff42794"
      },
      "source": [
        "## Visualize the prediction after model fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd1b40d-5974-4c53-aa68-97471ad6f918",
      "metadata": {
        "id": "6cd1b40d-5974-4c53-aa68-97471ad6f918"
      },
      "outputs": [],
      "source": [
        "##Predict tree crowns on a georeferenced image\n",
        "predicted_raster = model.predict_tile(raster_path, return_plot = True, patch_size=1000, patch_overlap=0.25, color=(255, 255, 0), thickness=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f1f1f4-9550-4e01-a775-8f90cbbe39bd",
      "metadata": {
        "id": "75f1f1f4-9550-4e01-a775-8f90cbbe39bd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(predicted_raster)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5752fc31-1340-4204-9934-c2e8e7f662b5",
      "metadata": {
        "id": "5752fc31-1340-4204-9934-c2e8e7f662b5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8fa637b2-220c-4c17-ac70-92710974a06c",
      "metadata": {
        "id": "8fa637b2-220c-4c17-ac70-92710974a06c"
      },
      "source": [
        "## Output and save prediction results for each image crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ecc787f-e3d9-437d-9201-09ac24b69b7c",
      "metadata": {
        "id": "3ecc787f-e3d9-437d-9201-09ac24b69b7c"
      },
      "outputs": [],
      "source": [
        "save_dir = os.path.join(savedir, 'pred_result')\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "results = model.evaluate(training_file, os.path.dirname(training_file), iou_threshold = 0.4, savedir= save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f360004-00e3-4e8a-909e-918d3bbb4963",
      "metadata": {
        "id": "9f360004-00e3-4e8a-909e-918d3bbb4963"
      },
      "outputs": [],
      "source": [
        "## Output and save validation results for each image crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9958f824-25ff-411d-a57a-3f1d81ea197c",
      "metadata": {
        "scrolled": true,
        "id": "9958f824-25ff-411d-a57a-3f1d81ea197c"
      },
      "outputs": [],
      "source": [
        "save_dir = os.path.join(savedir, 'valid_result')\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "results = model.evaluate(validation_file, os.path.dirname(validation_file), iou_threshold = 0.4, savedir= save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22989035-5aaa-468f-836f-47c8dd3d4497",
      "metadata": {
        "id": "22989035-5aaa-468f-836f-47c8dd3d4497"
      },
      "source": [
        "## Assessing the Quality of our Tree Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5933db49-0923-419b-8384-d7fca01bf2e0",
      "metadata": {
        "id": "5933db49-0923-419b-8384-d7fca01bf2e0"
      },
      "outputs": [],
      "source": [
        "#show assessment of results\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd50637a-023a-4d44-8f23-59a11dba49d6",
      "metadata": {
        "id": "dd50637a-023a-4d44-8f23-59a11dba49d6"
      },
      "outputs": [],
      "source": [
        "results['results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f40aa4b-5c30-4222-a72f-6e64cca2950d",
      "metadata": {
        "id": "9f40aa4b-5c30-4222-a72f-6e64cca2950d"
      },
      "outputs": [],
      "source": [
        "results['box_precision']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e60052e-ea87-4881-9f05-a58602742d03",
      "metadata": {
        "id": "3e60052e-ea87-4881-9f05-a58602742d03"
      },
      "outputs": [],
      "source": [
        "results[\"box_recall\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9ffcf2-956e-40fd-8b7a-ff865941ee0b",
      "metadata": {
        "id": "3b9ffcf2-956e-40fd-8b7a-ff865941ee0b"
      },
      "outputs": [],
      "source": [
        "results[\"class_recall\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef89f449-f3e9-4b62-a244-bd2e337b9693",
      "metadata": {
        "id": "ef89f449-f3e9-4b62-a244-bd2e337b9693"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "328a1c0e-f62d-4403-9183-55eef4f1d410",
      "metadata": {
        "id": "328a1c0e-f62d-4403-9183-55eef4f1d410"
      },
      "source": [
        "## Save and load the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acd3841-da3c-471f-95a1-03ea763d6724",
      "metadata": {
        "id": "3acd3841-da3c-471f-95a1-03ea763d6724"
      },
      "outputs": [],
      "source": [
        "#Save the fine-tuned model out to your storage\n",
        "save_model_dir = os.path.join(savedir, 'golf_course_deepforest.pt')\n",
        "torch.save(model.model.state_dict(),save_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c700fd-4958-42c1-aa83-b79c26039c24",
      "metadata": {
        "id": "87c700fd-4958-42c1-aa83-b79c26039c24"
      },
      "outputs": [],
      "source": [
        "#Bring existing model into environment\n",
        "fine_tuned_model = main.deepforest()\n",
        "fine_tuned_model.model.load_state_dict(torch.load(save_model_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fine-tuned model to Hugging Face"
      ],
      "metadata": {
        "id": "BwbYKPW1r9y2"
      },
      "id": "BwbYKPW1r9y2"
    },
    {
      "cell_type": "code",
      "source": [
        "#Install python libraries that allow you to connect with Hugging Face\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "2R9bSuBusGn-"
      },
      "id": "2R9bSuBusGn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input your Hugging Face username toke to authenticate your account\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bcvvHTv0sGdH"
      },
      "id": "bcvvHTv0sGdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Push fine-tuned model up to Hugging Face\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Set up repository details\n",
        "repo_name = \"deepforest_fine_tuning\"\n",
        "model_file = \"/content/golf_course_deepforest.pt\"\n",
        "\n",
        "# Create a new repo if it doesn't exist\n",
        "#api = HfApi()\n",
        "#api.create_repo(repo_name)\n",
        "\n",
        "# Upload model to Hugging Face\n",
        "api.upload_file(\n",
        "    path_or_fileobj=model_file,   # Path to your .pt file\n",
        "    repo_id=f\"jgillan/{repo_name}\",\n",
        "    path_in_repo=\"golf_course_deepforest.pt\"  # The name you want for the file on the Hub\n",
        ")\n"
      ],
      "metadata": {
        "id": "TtiYci6VuOQh"
      },
      "id": "TtiYci6VuOQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a model from Hugging Face and bring into Colab"
      ],
      "metadata": {
        "id": "A-HtLOST9Qju"
      },
      "id": "A-HtLOST9Qju"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the .pt file from Hugging Face\n",
        "model_file = hf_hub_download(repo_id=\"jgillan/deepforest_fine_tuning\", filename=\"golf_course_deepforest.pt\")\n",
        "\n",
        "fine_tuned_model = main.deepforest()\n",
        "fine_tuned_model.model.load_state_dict(torch.load(model_file))"
      ],
      "metadata": {
        "id": "fne1cd-x5cH9"
      },
      "id": "fne1cd-x5cH9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}